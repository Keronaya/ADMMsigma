---
title: "Regularized Precision Matrix Estimation via ADMM"
author: "Matt Galloway"
date: \today
output:
  html_document:
    toc: yes
    toc_depth: 2
  pdf_document:
    toc: yes
    toc_depth: 2
    number_sections: true
abstract: "`ADMMsigma` is an R package that estimates a penalized precision matrix via the alternating direction method of multipliers (ADMM) algorithm. This report will provide a brief overview of the algorithm and detail how it can be utilized to estimate precision matrices of joint normal distributions. In addition, examples and simulation results will be provided for `ADMMsigma`."
thanks: "**Contact**: gall0441@umn.edu."
geometry: margin = 1in
#fontfamily: mathpazo
#fontsize: 12pt
#spacing: double
header-includes:
   - \usepackage{bbm}
   - \usepackage{multirow}
   - \usepackage{graphicx}
   - \usepackage{amsmath}
   - \hypersetup{linkcolor=blue}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 40), echo = FALSE, tidy = TRUE, cache = TRUE)
```
```{r, message = F, warning = F, include = F, echo = F}

## load dependencies
library(magrittr)
library(readxl)
library(tables)
library(Hmisc)
library(pander)
library(ggsci)
library(gridExtra)
library(lmerTest)
library(lsmeans)
library(car)
library(lme4)
library(microbenchmark)
library(glasso)

panderOptions('digits', 2)
panderOptions('keep.trailing.zeros', FALSE)
panderOptions('table.split.table', Inf)

```

<br>\vspace{1cm}


# Introduction

Suppose we want to minimize $f(x) + g(z)$ subject to the constraint that $Ax + Bz = c$. For now, we will take $x \in \mathbb{R}^{n}, z \in \mathbb{R}^{m}, A \in \mathbb{R}^{p \times m}, B \in \mathbb{R}^{p \times m}, c \in \mathbb{R}^{p}$ -- though we will later consider cases where $x$ and $z$ are matrices. The *augmented lagrangian* is constructed as follows:

\[ L_{\rho}(x, z, y) = f(x) + g(z) + y^{T}(Ax + Bz - c) + \frac{\rho}{2}\left\| Ax + Bz - c \right\|_{2}^{2} \]

where $y \in \mathbb{R}^{p}$ is the lagrange multiplier. The optimal value is

\[ p^{*} = inf\left\{ f(x) + g(z) | Ax + Bz = c \right\} \]

Clearly, the minimization problem under the augmented lagrangian (RE-WORK) is equivalent to that of the usual lagrangian since any feasible point $(x, z)$ satisfies the constraint $\rho\left\| Ax + Bz - c \right\|_{2}^{2}/2 = 0$.

The ADMM algorithm consists of the following repeated iterations:

\begin{align}
  x^{k + 1} &:= \arg\min_{x}L_{\rho}(x, z^{k}, y^{k}) \\
  z^{k + 1} &:= \arg\min_{z}L_{\rho}(z^{k + 1}, z, y^{k}) \\
  y^{k + 1} &:= y^{k} + \rho(Ax^{k + 1} + Bz^{k + 1} - c)
\end{align}

A more complete introduction to the algorithm -- specifically how it arose out of *dual ascent* and *method of multipliers* -- can be found in Boyd, et al. (2011).

# Regularized Precision Matrix Estimation

We now consider the case where $X_{1}, ..., X_{n}$ are iid $N_{p}(\mu, \Sigma)$ and we are tasked with estimating the precision matrix, denoted $\Omega \equiv \Sigma^{-1}$. The maximum likelihood estimator for $\Omega$ is

\[ \hat{\Omega} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ Tr\left(S\Omega\right) - \log \det\left(\Omega \right) \right\} \]

where $S = \sum_{i = 1}^{n}(X_{i} - \bar{X})(X_{i} - \bar{X})^{T}/n$. It is straight forward to show that when the solution exists, $\hat{\Omega} = S^{-1}$.

We can further construct a penalized likelihood estimator by adding a penalty term, $P_{\lambda}\left( \Omega \right)$, to the likelihood:

\[ \hat{\Omega}_{\lambda} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ Tr\left(S\Omega\right) - \log \det\left(\Omega \right) + P_{\lambda}\left( \Omega \right) \right\} \]

Throughout the rest of this document we will take $P_{\lambda}\left( \Omega \right)$ to be $P_{\lambda}\left( \Omega \right) = \lambda\left[\frac{1 - \alpha}{2}\left\| \Omega \right|_{F}^{2} + \alpha\left\| \Omega \right\|_{1} \right]$ so that the full penalized likelihood is as follows:

\[ \hat{\Omega}_{\lambda} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ Tr\left(S\Omega\right) - \log \det\left(\Omega \right) + \lambda\left[\frac{1 - \alpha}{2}\left\| \Omega \right|_{F}^{2} + \alpha\left\| \Omega \right\|_{1} \right] \right\} \]

where $0 \leq \alpha \leq 1$, $\lambda > 0$, $0 < \eta < 2$, $\left\|\cdot \right\|_{F}^{2}$ is the Frobenius norm and we define $\left\|A \right\|_{1} = \sum_{i, j} \left| A_{ij} \right|$. This penalty is closely related to the elastic-net penalty explored by Hui Zou and Trevor Hastie \cite{4}. Clearly, when $\alpha = 0$ this reduces to a ridge-type penalty and when $\alpha = 1$ this reduces to a lasso-type penalty.

By letting $f$ be equal to the non-penalized likelihood and $g$ equal to $P_{\lambda}\left( \Omega \right)$, our goal is to minimize the full augmented lagrangian where the constraint is that $\Omega - Z$ is equal to zero:

\[ L_{\rho}(\Omega, Z, \Lambda) = f\left(\Omega\right) + g\left(Z\right) + Tr\left[\Lambda\left(\Omega - Z\right)\right] + \frac{\rho}{2}\left\|\Omega - Z\right\|_{F}^{2} \]


The ADMM algorithm for regularized precision matrix estimation is

\begin{align}
  \Omega^{k + 1} &= \arg\min_{S\Omega}\left\{ Tr\left(\Omega\right) - \log\det\left(\Omega\right) + Tr\left[\Lambda^{k}\left(\Omega - Z^{k}\right)\right] + \frac{\rho}{2}\left\| \Omega - Z^{k} \right\|_{F}^{2} \right\} \\
  Z^{k + 1} &= \arg\min_{Z}\left\{ \lambda\left[ \frac{1 - \alpha}{2}\left\| Z \right\|_{F}^{2} + \alpha\left\| Z \right\|_{1} \right] + Tr\left[\Lambda^{k}\left(\Omega^{k + 1} - Z\right)\right] + \frac{\rho}{2}\left\| \Omega^{k + 1} - Z \right\|_{F}^{2} \right\} \\
  \Lambda^{k + 1} &= \Lambda^{k} + \rho\left( \Omega^{k + 1} - Z^{k + 1} \right)
\end{align}


<br>\vspace{1cm}

## Condensed-Form ADMM

An alternate form of the ADMM algorithm can constructed by scaling the dual variable. Let us define $R^{k} = \Omega - Z^{k}$ and $U^{k} = \Lambda^{k}/\rho$. Then

\begin{align*}
  Tr\left[ \Lambda^{k}\left( \Omega - Z^{k} \right) \right] + \frac{\rho}{2}\left\| \Omega - Z^{k} \right\|_{F}^{2} &= Tr\left[ \Lambda^{k}R^{k} \right] + \frac{\rho}{2}\left\| R^{k} \right\|_{F}^{2} \\
  &= \frac{\rho}{2}\left\| R^{k} + \Lambda^{k}/\rho \right\|_{F}^{2} - \frac{\rho}{2}\left\| \Lambda^{k}/\rho \right\|_{F}^{2} \\
  &= \frac{\rho}{2}\left\| R^{k} + U^{k} \right\|_{F}^{2} - \frac{\rho}{2}\left\| U^{k} \right\|_{F}^{2}
\end{align*}

**The condensed-form can now be written as follows:**

\begin{align}
  \Omega^{k + 1} &= \arg\min_{\Omega}\left\{ Tr\left(\Omega\right) - \log\det\left(\Omega\right) + \frac{\rho}{2}\left\| \Omega - Z^{k} + U^{k} \right\|_{F}^{2} \right\} \\
  Z^{k + 1} &= \arg\min_{Z}\left\{ \lambda\left[ \frac{1 - \alpha}{2}\left\| Z \right\|_{F}^{2} + \alpha\left\| Z \right\|_{1} \right] + \frac{\rho}{2}\left\| \Omega^{k + 1} - Z + U^{k} \right\|_{F}^{2} \right\} \\
  U^{k + 1} &= U^{k} + \Omega^{k + 1} - Z^{k + 1}
\end{align}

More generally (in vector form),

\begin{align}
  x^{k + 1} &:= \arg\min_{x}\left\{ f(x) + \frac{\rho}{2}\left\| Ax + Bz^{k} - c + u^{k} \right\|_{2}^{2} \right\} \\
  z^{k + 1} &:= \arg\min_{z}\left\{ g(z) + \frac{\rho}{2}\left\| Ax^{k + 1} + Bz - c + u^{k} \right\|_{2}^{2} \right\} \\
  u^{k + 1} &:= u^{k} + Ax^{k + 1} + Bz^{k + 1} - c
\end{align}

Note that there are limitations to using this method. For instance, because the dual variable is scaled by $\rho$ (the step size), this form limits one to using a constant step size (without making further adjustments to $U^{k}$) -- a limitation that could prolong the convergence rate.

<br>\vspace{1cm}

## Algorithm

\begin{align*}
  \Omega^{k + 1} &= \arg\min_{\Omega}\left\{ Tr\left(\Omega\right) - \log\det\left(\Omega\right) + \frac{\rho}{2}\left\| \Omega - Z^{k} + U^{k} \right\|_{F}^{2} \right\} \\
  Z^{k + 1} &= \arg\min_{Z}\left\{ \lambda\left[ \frac{1 - \alpha}{2}\left\| Z \right\|_{F}^{2} + \alpha\left\| Z \right\|_{1} \right] + \frac{\rho}{2}\left\| \Omega^{k + 1} - Z + U^{k} \right\|_{F}^{2} \right\} \\
  U^{k + 1} &= U^{k} + \Omega^{k + 1} - Z^{k + 1}
\end{align*}

<br>\vspace{1cm}

1. Decompose $S + \rho(U^{k} - Z^{k}) = VQV^{T}$.

\[ \Omega^{k + 1} = \frac{1}{2\rho}V\left[ -Q + \left( Q^{2} + 4\rho I_{p} \right)^{1/2} \right]V^{T} \]

2. Elementwise soft-thresholding for all $i = 1,..., p$ and $j = 1,..., p$.

\begin{align*}
Z_{ij}^{k + 1} &= \frac{1}{\lambda(1 - \alpha) + \rho}sign\left(\Omega_{ij}^{k + 1} + U_{ij}^{k}\right)\left( \rho\left| \Omega_{ij}^{k + 1} + U_{ij}^{k} \right| - \lambda\eta\alpha \right)_{+} \\
&= \frac{1}{\lambda(1 - \alpha) + \rho}Soft\left( \rho(\Omega_{ij}^{k + 1} + U_{ij}^{k}\right), \lambda\eta\alpha)
\end{align*}

3. Update $U$.

\[ U^{k + 1} = U^{k} + \Omega^{k + 1} - Z^{k + 1} \]


<br>\vspace{1cm}

### Proof of (1):

(Work in progress.)



<br>\vspace{1cm}
**Code snippet**:


Note this is not the actual code. The real code is written in c++.

<br>\vspace{0.5cm}
```{r eval = FALSE, echo = TRUE}

# ridge penalized precision matrix function
RIDGEsigma = function(S, lam){
  
  # dimensions
  p = dim(S)[1]
  
  # gather eigen values of S (spectral decomposition)
  e.out = eigen(S, symmetric = TRUE)
  
  # augment eigen values for omega hat
  new.evs = (-e.out$val + sqrt(e.out$val^2 + 4*lam))/(2*lam)
  
  # compute omega hat for lambda (zero gradient equation)
  omega = tcrossprod(e.out$vec*rep(new.evs, each = p), e.out$vec)
  
  # compute gradient
  grad = S - qr.solve(omega) + lam * omega
  
  return(list(omega = omega, gradient = grad))
}

```
<br>\vspace{1cm}


### Proof of (2)

(Work in progress.)



<br>\vspace{1cm}
**Code snippet**:

Note this is not the actual code. The real code is written in c++.

<br>\vspace{0.5cm}
```{r eval = FALSE, echo = TRUE}

# ADMMsigma function
ADMMsigma = function(X = NULL, S = NULL, lam, alpha = 1, rho = 2, mu = 10, tau1 = 2, tau2 = 2, tol1 = 1e-4, tol2 = 1e-4, maxit = 1e3){
  
  # compute sample covariance matrix, if necessary
  if (is.null(S)){
    
    # covariance matrix
    n = dim(X)[1]
    S = (n - 1)/n*cov(X)
    
  }
  
  # allocate memory
  p = dim(S)[1]
  criterion = TRUE
  iter = lik = s = r = eps1 = eps2 = 0
  new.Z = Y = Omega = matrix(0, nrow = p, ncol = p)
  
  # loop until convergence
  while (criterion && (iter <= maxit)){
    
    # ridge equation (1)
    # gather eigen values (spectral decomposition)
    Z = new.Z
    Omega = sigma_ridge(S + Y - rho*Z, lam = rho)$omega
    
    # penalty equation (2)
    # soft-thresholding
    new.Z = soft(Y + rho*Omega, lam*alpha)/(lam*(1 - alpha) + rho)
    
    # update U (3)
    Y = Y + rho*(Omega - new.Z)
    
    # calculate new rho
    s = sqrt(sum((rho*(new.Z - Z))^2))
    r = sqrt(sum((Omega - new.Z)^2))
    rho = rho*(tau1*(r > mu*s) + (s > mu*r)/tau2 + (s/mu <= r & r <= mu*s))
    iter = iter + 1
    
    # stopping criterion
    eps1 = p*tol1 + tol2*max(sqrt(sum(Omega^2)), sqrt(sum(new.Z^2)))
    eps2 = p*tol1 + tol2*sqrt(sum(Y^2))
    criterion = (r >= eps1 || s >= eps2)
  
  }
  return(list(Iterations = iter, Omega = Omega))
}


```
<br>\vspace{1cm}



<br>\vspace{1cm}

# R Package

## Installation

<br>\vspace{0.5cm}
```{r, eval = FALSE, echo = TRUE}
# The easiest way to install is from the development version from GitHub:
# install.packages("devtools")
devtools::install_github("MGallow/ADMMsigma")
```
<br>\vspace{0.5cm}

If there are any issues/bugs, please let me know: [github](https://github.com/MGallow/ADMMsigma/issues). You can also contact me via my [website](http://users.stat.umn.edu/~gall0441/). Pull requests are welcome!



<br>\vspace{1cm}

## Usage


<br>\vspace{0.5cm}
```{r, message = FALSE, echo = TRUE}
library(ADMMsigma)

#  generate data from a dense matrix for example
# first compute covariance matrix
S = matrix(0, nrow = 5, ncol = 5)

for (i in 1:5){
  for (j in 1:5){
    S[i, j] = 0.9^(i != j)
  }
}

# generate 100x5 matrix with rows drawn from iid N_p(0, S)
Z = matrix(rnorm(100*10), nrow = 100, ncol = 5)
out = eigen(S, symmetric = TRUE)
S.sqrt = out$vectors %*% diag(out$values^0.5) %*% t(out$vectors)
X = Z %*% S.sqrt


# elastic-net type penalty (use CV for optimal lambda and alpha)
ADMMsigma(X)

# ridge penalty (use CV for optimal lambda)
ADMMsigma(X, alpha = 0)

# lasso penalty (lam = 0.1)
ADMMsigma(X, lam = 0.1, alpha = 1)

# ridge penalty no ADMM
RIDGEsigma(X, lam = 10^seq(-8, 8, 0.01))

# produce CV heat map for ADMMsigma
ADMMsigma(X) %>% plot

# produce CV heat map for RIDGEsigma
RIDGEsigma(X, lam = 10^seq(-8, 8, 0.01)) %>% plot

```
<br>\vspace{0.5cm}


## Benchmark

### Computer Specs:

 - MacBook Pro (Late 2016)
 - Processor: 2.9 GHz Intel Core i5
 - Memory: 8GB 2133 MHz
 - Graphics: Intel Iris Graphics 550


<br>\vspace{0.5cm}
```{r, message = FALSE, echo = TRUE}

# generate data from tri-diagonal (sparse) matrix for example
# first compute covariance matrix (can confirm inverse is tri-diagonal)
S = matrix(0, nrow = 10, ncol = 10)

for (i in 1:10){
  for (j in 1:10){
    S[i, j] = 0.7^(abs(i - j))
  }
}

# generate 1000x100 matrix with rows drawn from iid N_p(0, S)
Z = matrix(rnorm(100*10), nrow = 100, ncol = 10)
out = eigen(S, symmetric = TRUE)
S.sqrt = out$vectors %*% diag(out$values^0.5) %*% t(out$vectors)
X = Z %*% S.sqrt


# glasso
microbenchmark(glasso(s = S, rho = 0.1), times = 5)

# benchmark ADMMsigma - default tolerance
microbenchmark(ADMMsigma(S = S, lam = 0.1, alpha = 1, tol1 = 1e-4, tol2 = 1e-4), times = 5)

# benchmark ADMMsigma - tolerance 1e-8
microbenchmark(ADMMsigma(S = S, lam = 0.1, alpha = 1, tol1 = 1e-8, tol2 = 1e-8), times = 5)

# benchmark ADMMsigma CV - likelihood convergence criteria
microbenchmark(ADMMsigma(X, crit = "loglik"), times = 5)

# benchmark ADMMsigma CV
microbenchmark(ADMMsigma(X, lam = 10^seq(-8, 8, 0.1)), times = 5)

```
<br>\vspace{0.5cm}


<br>\newpage

\begin{thebibliography}{9}

\bibitem{1}
  Boyd, Stephen, et al. "Distributed optimization and statistical learning via the alternating direction method of multipliers." Foundations and Trends® in Machine Learning 3.1 (2011): 1-122.
  
\bibitem{2}
  Polson, Nicholas G., James G. Scott, and Brandon T. Willard. "Proximal algorithms in statistics and machine learning." Statistical Science 30.4 (2015): 559-581.
  
\bibitem{3}
  Marjanovic, Goran, and Victor Solo. "On $ l_q $ optimization and matrix completion." IEEE Transactions on signal processing 60.11 (2012): 5714-5724.
  
\bibitem{4}
  Zou, Hui, and Trevor Hastie. "Regularization and variable selection via the elastic net." Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67.2 (2005): 301-320.

\end{thebibliography}

<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Regularized Precision Matrix Estimation via ADMM • ADMMsigma</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><meta property="og:title" content="Regularized Precision Matrix Estimation via ADMM">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">ADMMsigma</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">2.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li>
  <a href="../articles/details.html">Details</a>
</li>
<li>
  <a href="../reference/index.html">Functions</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Simulations.html">Simulations</a>
    </li>
    <li>
      <a href="../articles/Benchmark.html">Benchmark</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Related Packages
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="https://mgallow.github.io/OMEGA/">OMEGA</a>
    </li>
    <li>
      <a href="https://mgallow.github.io/GLASSOO/">GLASSOO</a>
    </li>
    <li>
      <a href="https://mgallow.github.io/CVglasso/">CVglasso</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/MGallow/ADMMsigma/">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Regularized Precision Matrix Estimation via ADMM</h1>
                        <h4 class="author">Matt Galloway</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/MGallow/ADMMsigma/blob/master/vignettes/ADMMsigma.Rmd"><code>vignettes/ADMMsigma.Rmd</code></a></small>
      <div class="hidden name"><code>ADMMsigma.Rmd</code></div>

    </div>

    
        <div class="abstract">
      <p class="abstract">Abstract</p>
      <code>ADMMsigma</code> is an R package that estimates a penalized precision matrix via the alternating direction method of multipliers (ADMM) algorithm. It currently supports a general elastic-net penalty that allows for both ridge and lasso-type penalties as special cases. This report will provide a brief overview of the algorithm, discuss the functions contained in <code>ADMMsigma</code>, and provide simulation results.
    </div>
    
<p><br></p>
<div id="introduction" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h1>
<p>Suppose we want to solve the following optimization problem:</p>
<span class="math display">\[\begin{align*}
  \mbox{minimize } f(x) + g(z) \\
  \mbox{subject to } Ax + Bz = c
\end{align*}\]</span>
<p>where <span class="math inline">\(x \in \mathbb{R}^{n}, z \in \mathbb{R}^{m}, A \in \mathbb{R}^{p \times n}, B \in \mathbb{R}^{p \times m}\)</span>, and <span class="math inline">\(c \in \mathbb{R}^{p}\)</span>. Following <span class="citation">Boyd et al. (2011)</span>, the optimization problem will be introduced in vector form though we will later consider cases where <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> are matrices. We will also assume <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are convex functions. Optimization problems like this arise naturally in several statistics and machine learning applications – particularly regularization methods. For instance, we could take <span class="math inline">\(f\)</span> to be the squared error loss, <span class="math inline">\(g\)</span> to be the <span class="math inline">\(l_{2}\)</span>-norm, <span class="math inline">\(c\)</span> to be equal to zero and <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> to be identity matrices to solve the ridge regression optimization problem.</p>
<p>The <em>augmented lagrangian</em> is constructed as follows:</p>
<p><span class="math display">\[ L_{\rho}(x, z, y) = f(x) + g(z) + y^{T}(Ax + Bz - c) + \frac{\rho}{2}\left\| Ax + Bz - c \right\|_{2}^{2} \]</span></p>
<p>where <span class="math inline">\(y \in \mathbb{R}^{p}\)</span> is the lagrange multiplier and <span class="math inline">\(\rho &gt; 0\)</span> is a scalar. Clearly any minimizer, <span class="math inline">\(p^{*}\)</span>, under the augmented lagrangian is equivalent to that of the lagrangian since any feasible point <span class="math inline">\((x, z)\)</span> satisfies the constraint <span class="math inline">\(\rho\left\| Ax + Bz - c \right\|_{2}^{2}/2 = 0\)</span>.</p>
<p><span class="math display">\[ p^{*} = inf\left\{ f(x) + g(z) | Ax + Bz = c \right\} \]</span></p>
<p>The alternating direction method of multipliers (ADMM) algorithm consists of the following repeated iterations:</p>
<span class="math display">\[\begin{align}
  x^{k + 1} &amp;:= \arg\min_{x}L_{\rho}(x, z^{k}, y^{k}) \\
  z^{k + 1} &amp;:= \arg\min_{z}L_{\rho}(x^{k + 1}, z, y^{k}) \\
  y^{k + 1} &amp;:= y^{k} + \rho(Ax^{k + 1} + Bz^{k + 1} - c)
\end{align}\]</span>
<p>A more complete introduction to the algorithm – specifically how it arose out of <em>dual ascent</em> and <em>method of multipliers</em> – can be found in <span class="citation">Boyd et al. (2011)</span>.</p>
<p><br></p>
</div>
<div id="regularized-precision-matrix-estimation" class="section level1">
<h1 class="hasAnchor">
<a href="#regularized-precision-matrix-estimation" class="anchor"></a>Regularized Precision Matrix Estimation</h1>
<p>Now consider the case where <span class="math inline">\(X_{1}, ..., X_{n}\)</span> are iid <span class="math inline">\(N_{p}(\mu, \Sigma)\)</span> random variables and we are tasked with estimating the precision matrix, denoted <span class="math inline">\(\Omega \equiv \Sigma^{-1}\)</span>. The maximum likelihood estimator for <span class="math inline">\(\Omega\)</span> is</p>
<p><span class="math display">\[ \hat{\Omega}_{MLE} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ Tr\left(S\Omega\right) - \log \det\left(\Omega \right) \right\} \]</span></p>
<p>where <span class="math inline">\(S = \sum_{i = 1}^{n}(X_{i} - \bar{X})(X_{i} - \bar{X})^{T}/n\)</span> and <span class="math inline">\(\bar{X}\)</span> is the sample mean. By setting the gradient equal to zero, we can show that when the solution exists, <span class="math inline">\(\hat{\Omega}_{MLE} = S^{-1}\)</span>.</p>
<p>As in regression settings, we can construct a <em>penalized</em> likelihood estimator by adding a penalty term, <span class="math inline">\(P\left( \Omega \right)\)</span>, to the likelihood:</p>
<p><span class="math display">\[ \hat{\Omega} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ Tr\left(S\Omega\right) - \log \det\left(\Omega \right) + P\left( \Omega \right) \right\} \]</span></p>
<p><span class="math inline">\(P\left( \Omega \right)\)</span> is often of the form <span class="math inline">\(P\left(\Omega \right) = \lambda\|\Omega \|_{F}^{2}\)</span> or <span class="math inline">\(P\left(\Omega \right) = \|\Omega\|_{1}\)</span> where <span class="math inline">\(\lambda &gt; 0\)</span>, <span class="math inline">\(\left\|\cdot \right\|_{F}^{2}\)</span> is the Frobenius norm and we define <span class="math inline">\(\left\|A \right\|_{1} = \sum_{i, j} \left| A_{ij} \right|\)</span>. These penalties are the ridge and lasso, respectively. We will, instead, take <span class="math inline">\(P\left( \Omega \right) = \lambda\left[\frac{1 - \alpha}{2}\left\| \Omega \right|_{F}^{2} + \alpha\left\| \Omega \right\|_{1} \right]\)</span> so that the full penalized likelihood is</p>
<p><span class="math display">\[ \hat{\Omega} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ Tr\left(S\Omega\right) - \log \det\left(\Omega \right) + \lambda\left[\frac{1 - \alpha}{2}\left\| \Omega \right|_{F}^{2} + \alpha\left\| \Omega \right\|_{1} \right] \right\} \]</span></p>
<p>where <span class="math inline">\(0 \leq \alpha \leq 1\)</span>. This <em>elastic-net</em> penalty was explored by Hui Zou and Trevor Hastie <span class="citation">(Zou and Hastie 2005)</span> and is identical to the penalty used in the popular penalized regression package <code>glmnet</code>. Clearly, when <span class="math inline">\(\alpha = 0\)</span> the elastic-net reduces to a ridge-type penalty and when <span class="math inline">\(\alpha = 1\)</span> it reduces to a lasso-type penalty.</p>
<p>By letting <span class="math inline">\(f\)</span> be equal to the non-penalized likelihood and <span class="math inline">\(g\)</span> equal to <span class="math inline">\(P\left( \Omega \right)\)</span>, our goal is to minimize the full augmented lagrangian where the constraint is that <span class="math inline">\(\Omega - Z\)</span> is equal to zero:</p>
<p><span class="math display">\[ L_{\rho}(\Omega, Z, \Lambda) = f\left(\Omega\right) + g\left(Z\right) + Tr\left[\Lambda\left(\Omega - Z\right)\right] + \frac{\rho}{2}\left\|\Omega - Z\right\|_{F}^{2} \]</span></p>
<p>The ADMM algorithm for estimating the penalized precision matrix in this problem is</p>
<span class="math display">\[\begin{align}
  \Omega^{k + 1} &amp;= \arg\min_{\Omega}\left\{ Tr\left(S\Omega\right) - \log\det\left(\Omega\right) + Tr\left[\Lambda^{k}\left(\Omega - Z^{k}\right)\right] + \frac{\rho}{2}\left\| \Omega - Z^{k} \right\|_{F}^{2} \right\} \\
  Z^{k + 1} &amp;= \arg\min_{Z}\left\{ \lambda\left[ \frac{1 - \alpha}{2}\left\| Z \right\|_{F}^{2} + \alpha\left\| Z \right\|_{1} \right] + Tr\left[\Lambda^{k}\left(\Omega^{k + 1} - Z\right)\right] + \frac{\rho}{2}\left\| \Omega^{k + 1} - Z \right\|_{F}^{2} \right\} \\
  \Lambda^{k + 1} &amp;= \Lambda^{k} + \rho\left( \Omega^{k + 1} - Z^{k + 1} \right)
\end{align}\]</span>
<p><br></p>
<div id="scaled-form-admm" class="section level2">
<h2 class="hasAnchor">
<a href="#scaled-form-admm" class="anchor"></a>Scaled-Form ADMM</h2>
<p>An alternate form of the ADMM algorithm can constructed by scaling the dual variable (<span class="math inline">\(\Lambda^{k}\)</span>). Let us define <span class="math inline">\(R^{k} = \Omega - Z^{k}\)</span> and <span class="math inline">\(U^{k} = \Lambda^{k}/\rho\)</span>.</p>
<span class="math display">\[\begin{align*}
  Tr\left[ \Lambda^{k}\left( \Omega - Z^{k} \right) \right] + \frac{\rho}{2}\left\| \Omega - Z^{k} \right\|_{F}^{2} &amp;= Tr\left[ \Lambda^{k}R^{k} \right] + \frac{\rho}{2}\left\| R^{k} \right\|_{F}^{2} \\
  &amp;= \frac{\rho}{2}\left\| R^{k} + \Lambda^{k}/\rho \right\|_{F}^{2} - \frac{\rho}{2}\left\| \Lambda^{k}/\rho \right\|_{F}^{2} \\
  &amp;= \frac{\rho}{2}\left\| R^{k} + U^{k} \right\|_{F}^{2} - \frac{\rho}{2}\left\| U^{k} \right\|_{F}^{2}
\end{align*}\]</span>
<p>Therefore, the condensed-form ADMM algorithm can now be written as</p>
<span class="math display">\[\begin{align}
  \Omega^{k + 1} &amp;= \arg\min_{\Omega}\left\{ Tr\left(S\Omega\right) - \log\det\left(\Omega\right) + \frac{\rho}{2}\left\| \Omega - Z^{k} + U^{k} \right\|_{F}^{2} \right\} \\
  Z^{k + 1} &amp;= \arg\min_{Z}\left\{ \lambda\left[ \frac{1 - \alpha}{2}\left\| Z \right\|_{F}^{2} + \alpha\left\| Z \right\|_{1} \right] + \frac{\rho}{2}\left\| \Omega^{k + 1} - Z + U^{k} \right\|_{F}^{2} \right\} \\
  U^{k + 1} &amp;= U^{k} + \Omega^{k + 1} - Z^{k + 1}
\end{align}\]</span>
<p>And more generally (in vector form),</p>
<span class="math display">\[\begin{align}
  x^{k + 1} &amp;= \arg\min_{x}\left\{ f(x) + \frac{\rho}{2}\left\| Ax + Bz^{k} - c + u^{k} \right\|_{2}^{2} \right\} \\
  z^{k + 1} &amp;= \arg\min_{z}\left\{ g(z) + \frac{\rho}{2}\left\| Ax^{k + 1} + Bz - c + u^{k} \right\|_{2}^{2} \right\} \\
  u^{k + 1} &amp;= u^{k} + Ax^{k + 1} + Bz^{k + 1} - c
\end{align}\]</span>
<p>Note that there are limitations to using this method. Because the dual variable is scaled by <span class="math inline">\(\rho\)</span> (the step size), this form limits one to using a constant step size without making further adjustments to <span class="math inline">\(U^{k}\)</span>. It has been shown in the literature that a dynamic step size can significantly reduce the number of iterations required for convergence.</p>
<p><br></p>
</div>
<div id="algorithm" class="section level2">
<h2 class="hasAnchor">
<a href="#algorithm" class="anchor"></a>Algorithm</h2>
<span class="math display">\[\begin{align*}
  \Omega^{k + 1} &amp;= \arg\min_{\Omega}\left\{ Tr\left(S\Omega\right) - \log\det\left(\Omega\right) + Tr\left[\Lambda^{k}\left(\Omega - Z^{k}\right)\right] + \frac{\rho}{2}\left\| \Omega - Z^{k} \right\|_{F}^{2} \right\} \\
  Z^{k + 1} &amp;= \arg\min_{Z}\left\{ \lambda\left[ \frac{1 - \alpha}{2}\left\| Z \right\|_{F}^{2} + \alpha\left\| Z \right\|_{1} \right] + Tr\left[\Lambda^{k}\left(\Omega^{k + 1} - Z\right)\right] + \frac{\rho}{2}\left\| \Omega^{k + 1} - Z \right\|_{F}^{2} \right\} \\
  \Lambda^{k + 1} &amp;= \Lambda^{k} + \rho\left( \Omega^{k + 1} - Z^{k + 1} \right)
\end{align*}\]</span>
<p><br></p>
<p>Initialize <span class="math inline">\(Z^{0}, \Lambda^{0}\)</span>, and <span class="math inline">\(\rho\)</span>. Iterate the following three steps until convergence:</p>
<ol style="list-style-type: decimal">
<li>Decompose <span class="math inline">\(S + \Lambda^{k} - \rho Z^{k} = VQV^{T}\)</span> via spectral decomposition.</li>
</ol>
<p><span class="math display">\[ \Omega^{k + 1} = \frac{1}{2\rho}V\left[ -Q + \left( Q^{2} + 4\rho I_{p} \right)^{1/2} \right]V^{T} \]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Elementwise soft-thresholding for all <span class="math inline">\(i = 1,..., p\)</span> and <span class="math inline">\(j = 1,..., p\)</span>.</li>
</ol>
<span class="math display">\[\begin{align*}
Z_{ij}^{k + 1} &amp;= \frac{1}{\lambda(1 - \alpha) + \rho}Sign\left(\rho\Omega_{ij}^{k + 1} + \Lambda_{ij}^{k}\right)\left( \left| \rho\Omega_{ij}^{k + 1} + \Lambda_{ij}^{k} \right| - \lambda\alpha \right)_{+} \\
&amp;= \frac{1}{\lambda(1 - \alpha) + \rho}Soft\left(\rho\Omega_{ij}^{k + 1} + \Lambda_{ij}^{k}, \lambda\alpha\right)
\end{align*}\]</span>
<ol start="3" style="list-style-type: decimal">
<li>Update <span class="math inline">\(\Lambda^{k + 1}\)</span>.</li>
</ol>
<p><span class="math display">\[ \Lambda^{k + 1} = \Lambda^{k} + \rho\left( \Omega^{k + 1} - Z^{k + 1} \right) \]</span></p>
<p><br></p>
<div id="proof-of-1" class="section level3">
<h3 class="hasAnchor">
<a href="#proof-of-1" class="anchor"></a>Proof of (1):</h3>
<p><span class="math display">\[ \Omega^{k + 1} = \arg\min_{\Omega}\left\{ Tr\left(S\Omega\right) - \log\det\left(\Omega\right) + Tr\left[\Lambda^{k}\left(\Omega - Z^{k}\right)\right] + \frac{\rho}{2}\left\| \Omega - Z^{k} \right\|_{F}^{2} \right\} \]</span></p>
<span class="math display">\[\begin{align*}
  \nabla_{\Omega}&amp;\left\{ Tr\left(S\Omega\right) - \log\det\left(\Omega\right) + Tr\left[\Lambda^{k}\left(\Omega - Z^{k}\right)\right] + \frac{\rho}{2}\left\| \Omega - Z^{k} \right\|_{F}^{2} \right\} \\
  &amp;= S - \Omega^{-1} + \Lambda^{k} + \rho\left( \Omega - Z^{k} \right)
\end{align*}\]</span>
<p>Set the gradient equal to zero and decompose <span class="math inline">\(\Omega = VDV^{T}\)</span> where <span class="math inline">\(D\)</span> is a diagonal matrix with diagonal elements equal to the eigen values of <span class="math inline">\(\Omega\)</span> and <span class="math inline">\(V\)</span> is the matrix with corresponding eigen vectors as columns.</p>
<p><span class="math display">\[ S + \Lambda^{k} - \rho Z^{k} = \Omega^{-1} - \rho \Omega = VD^{-1}V^{T} - \rho VDV^{T} =  V\left(D^{-1} - \rho D\right)V^{T} \]</span></p>
<p>This equivalence implies that</p>
<p><span class="math display">\[ \phi_{j}\left( S + \Lambda^{k} - \rho Z^{k} \right) = \frac{1}{\phi_{j}(\Omega^{k + 1})} - \rho\phi_{j}(\Omega^{k + 1}) \]</span></p>
<p>where <span class="math inline">\(\phi_{j}(\cdot)\)</span> is the <span class="math inline">\(j\)</span>th eigen value.</p>
<span class="math display">\[\begin{align*}
  &amp;\Rightarrow \rho\phi_{j}^{2}(\Omega^{k + 1}) + \phi_{j}\left( S + \Lambda^{k} - \rho Z^{k} \right)\phi_{j}(\Omega^{k + 1}) - 1 = 0 \\
  &amp;\Rightarrow \phi_{j}(\Omega^{k + 1}) = \frac{-\phi_{j}(S + \Lambda^{k} - \rho Z^{k}) \pm \sqrt{\phi_{j}^{2}(S + \Lambda^{k} - \rho Z^{k}) + 4\rho}}{2\rho}
\end{align*}\]</span>
<p>In summary, if we decompose <span class="math inline">\(S + \Lambda^{k} - \rho Z^{k} = VQV^{T}\)</span> then</p>
<p><span class="math display">\[ \Omega^{k + 1} = \frac{1}{2\rho}V\left[ -Q + (Q^{2} + 4\rho I_{p})^{1/2}\right] V^{T} \]</span></p>
<p><br></p>
</div>
<div id="proof-of-2" class="section level3">
<h3 class="hasAnchor">
<a href="#proof-of-2" class="anchor"></a>Proof of (2)</h3>
<p><span class="math display">\[ Z^{k + 1} = \arg\min_{Z}\left\{ \lambda\left[ \frac{1 - \alpha}{2}\left\| Z \right\|_{F}^{2} + \alpha\left\| Z \right\|_{1} \right] + Tr\left[\Lambda^{k}\left(\Omega^{k + 1} - Z\right)\right] + \frac{\rho}{2}\left\| \Omega^{k + 1} - Z \right\|_{F}^{2} \right\} \]</span></p>
<span class="math display">\[\begin{align*}
  \partial&amp;\left\{ \lambda\left[ \frac{1 - \alpha}{2}\left\| Z \right\|_{F}^{2} + \alpha\left\| Z \right\|_{1} \right] + Tr\left[\Lambda^{k}\left(\Omega^{k + 1} - Z\right)\right] + \frac{\rho}{2}\left\| \Omega^{k + 1} - Z \right\|_{F}^{2} \right\} \\
  &amp;= \partial\left\{ \lambda\left[ \frac{1 - \alpha}{2}\left\| Z \right\|_{F}^{2} + \alpha\left\| Z \right\|_{1} \right] + Tr\left[\Lambda^{k}\left(\Omega^{k + 1} - Z\right)\right]\right\} + \nabla_{\Omega}\left\{\frac{\rho}{2}\left\| \Omega^{k + 1} - Z \right\|_{F}^{2} \right\} \\
  &amp;= \lambda(1 - \alpha)Z + Sign(Z)\lambda\alpha - \Lambda^{k} - \rho\left( \Omega^{k + 1} - Z \right)
\end{align*}\]</span>
<p>where <span class="math inline">\(Sign(Z)\)</span> is the elementwise Sign operator. By setting the gradient/sub-differential equal to zero, we arrive at the following equivalence:</p>
<p><span class="math display">\[ Z_{ij} = \frac{1}{\lambda(1 - \alpha) + \rho}\left( \rho \Omega_{ij}^{k + 1} + \Lambda_{ij}^{k} - Sign(Z_{ij})\lambda\alpha \right) \]</span></p>
<p>for all <span class="math inline">\(i = 1,..., p\)</span> and <span class="math inline">\(j = 1,..., p\)</span>. We observe two scenarios:</p>
<ul>
<li>If <span class="math inline">\(Z_{ij} &gt; 0\)</span> then</li>
</ul>
<p><span class="math display">\[ \rho\Omega_{ij}^{k + 1} + \Lambda_{ij}^{k} &gt; \lambda\alpha \]</span></p>
<ul>
<li>If <span class="math inline">\(Z_{ij} &lt; 0\)</span> then</li>
</ul>
<p><span class="math display">\[ \rho\Omega_{ij}^{k + 1} + \Lambda_{ij}^{k} &lt; -\lambda\alpha \]</span></p>
<p>This implies that <span class="math inline">\(Sign(Z_{ij}) = Sign(\rho\Omega_{ij}^{k + 1} + \Lambda_{ij}^{k})\)</span>. Putting all the pieces together, we arrive at</p>
<span class="math display">\[\begin{align*}
Z_{ij}^{k + 1} &amp;= \frac{1}{\lambda(1 - \alpha) + \rho}Sign\left(\rho\Omega_{ij}^{k + 1} + \Lambda_{ij}^{k}\right)\left( \left| \rho\Omega_{ij}^{k + 1} + \Lambda_{ij}^{k} \right| - \lambda\alpha \right)_{+} \\
&amp;= \frac{1}{\lambda(1 - \alpha) + \rho}Soft\left(\rho\Omega_{ij}^{k + 1} + \Lambda_{ij}^{k}, \lambda\alpha\right)
\end{align*}\]</span>
<p>where <span class="math inline">\(Soft\)</span> is the soft-thresholding function.</p>
<p><br><br></p>
</div>
</div>
</div>
<div id="admmsigma-r-package" class="section level1">
<h1 class="hasAnchor">
<a href="#admmsigma-r-package" class="anchor"></a><code>ADMMsigma</code> R Package</h1>
<div id="installation" class="section level2">
<h2 class="hasAnchor">
<a href="#installation" class="anchor"></a>Installation</h2>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The easiest way to install is from CRAN</span>
<span class="kw">install.packages</span>(<span class="st">"ADMMsigma"</span>)

<span class="co"># You can also install the development</span>
<span class="co"># version from GitHub:</span>
<span class="co"># install.packages('devtools')</span>
devtools<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/devtools/topics/install_github">install_github</a></span>(<span class="st">"MGallow/ADMMsigma"</span>)</code></pre></div>
<p><br></p>
<p>If there are any issues/bugs, please let me know: <a href="https://github.com/MGallow/ADMMsigma/issues">github</a>. You can also contact me via my <a href="http://users.stat.umn.edu/~gall0441/">website</a>. Pull requests are welcome!</p>
<p><br></p>
<p>A (possibly incomplete) list of functions contained in the package can be found below:</p>
<ul>
<li><p><code><a href="../reference/ADMMsigma.html">ADMMsigma()</a></code> computes the estimated precision matrix (ridge, lasso, and elastic-net type regularization optional)</p></li>
<li><p><code><a href="../reference/RIDGEsigma.html">RIDGEsigma()</a></code> computes the estimated ridge penalized precision matrix via closed-form solution</p></li>
<li><p><code>plot.ADMMsigma()</code> produces a heat map or line graph for cross validation errors</p></li>
<li><p><code>plot.RIDGEsigma()</code> produces a heat map or line graph for cross validation errors</p></li>
</ul>
<p><br></p>
</div>
<div id="usage" class="section level2">
<h2 class="hasAnchor">
<a href="#usage" class="anchor"></a>Usage</h2>
<p>We will first generate data from a sparse, tri-diagonal precision matrix and denote it as Omega.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ADMMsigma)

<span class="co"># generate data from a sparse matrix</span>
<span class="co"># first compute covariance matrix</span>
S =<span class="st"> </span><span class="kw">matrix</span>(<span class="fl">0.7</span>, <span class="dt">nrow =</span> <span class="dv">5</span>, <span class="dt">ncol =</span> <span class="dv">5</span>)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {
    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {
        S[i, j] =<span class="st"> </span>S[i, j]<span class="op">^</span><span class="kw">abs</span>(i <span class="op">-</span><span class="st"> </span>j)
    }
}

<span class="co"># print oracle precision matrix</span>
<span class="co"># (shrinkage might be useful)</span>
(<span class="dt">Omega =</span> <span class="kw">qr.solve</span>(S) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">3</span>))</code></pre></div>
<pre><code>##        [,1]   [,2]   [,3]   [,4]   [,5]
## [1,]  1.961 -1.373  0.000  0.000  0.000
## [2,] -1.373  2.922 -1.373  0.000  0.000
## [3,]  0.000 -1.373  2.922 -1.373  0.000
## [4,]  0.000  0.000 -1.373  2.922 -1.373
## [5,]  0.000  0.000  0.000 -1.373  1.961</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate 1000 x 5 matrix with rows</span>
<span class="co"># drawn from iid N_p(0, S)</span>
Z =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">100</span> <span class="op">*</span><span class="st"> </span><span class="dv">5</span>), <span class="dt">nrow =</span> <span class="dv">100</span>, <span class="dt">ncol =</span> <span class="dv">5</span>)
out =<span class="st"> </span><span class="kw">eigen</span>(S, <span class="dt">symmetric =</span> <span class="ot">TRUE</span>)
S.sqrt =<span class="st"> </span>out<span class="op">$</span>vectors <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(out<span class="op">$</span>values<span class="op">^</span><span class="fl">0.5</span>) <span class="op">%*%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">t</span>(out<span class="op">$</span>vectors)
X =<span class="st"> </span>Z <span class="op">%*%</span><span class="st"> </span>S.sqrt

<span class="co"># snap shot of data</span>
<span class="kw">head</span>(X)</code></pre></div>
<pre><code>##            [,1]       [,2]       [,3]       [,4]       [,5]
## [1,] -0.5717795  0.4033083  0.4862421 -0.3730276 -0.6997268
## [2,]  0.1677901 -0.7857414 -0.5949253 -1.0237384 -1.1056246
## [3,]  0.8291701  0.6128357  0.2038505  1.8371358  1.5902374
## [4,] -0.5383905 -0.5632869 -0.1762810 -1.6762049 -1.4923724
## [5,]  0.2280512  0.7799799 -0.2662428  0.1771806  0.0605152
## [6,] -1.2244974  1.2247869  0.8376686  1.7796413  0.2560312</code></pre>
<p><br></p>
<p>As described earlier in the report, the maximum likelihood estimator (MLE) for Omega is the inverse of the sample precision matrix <span class="math inline">\(S^{-1} = \left[\sum_{i = 1}^{n}(X_{i} - \bar{X})(X_{i} - \bar{X})^{T}/n \right]^{-1}\)</span>:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># print inverse of sample precision</span>
<span class="co"># matrix (perhaps a bad estimate)</span>
(<span class="kw">qr.solve</span>(<span class="kw">cov</span>(X) <span class="op">*</span><span class="st"> </span>(<span class="kw">nrow</span>(X) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span><span class="kw">nrow</span>(X)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">round</span>(<span class="dv">5</span>))</code></pre></div>
<pre><code>##          [,1]     [,2]     [,3]     [,4]     [,5]
## [1,]  1.95121 -1.19264 -0.15640  0.09491  0.00569
## [2,] -1.19264  2.85274 -1.45104 -0.13297 -0.31207
## [3,] -0.15640 -1.45104  3.03918 -1.23656  0.22236
## [4,]  0.09491 -0.13297 -1.23656  2.98569 -1.40361
## [5,]  0.00569 -0.31207  0.22236 -1.40361  2.14536</code></pre>
<p><br></p>
<p>However, because Omega (known as the <em>oracle</em>) is sparse, a shrinkage estimator will perhaps perform better than the sample estimator. Below we construct various penalized estimators:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># elastic-net type penalty (set tolerance</span>
<span class="co"># to 1e-8)</span>
<span class="kw"><a href="../reference/ADMMsigma.html">ADMMsigma</a></span>(X, <span class="dt">tol.abs =</span> <span class="fl">1e-08</span>, <span class="dt">tol.rel =</span> <span class="fl">1e-08</span>)</code></pre></div>
<pre><code>## 
## Call: ADMMsigma(X = X, tol.abs = 1e-08, tol.rel = 1e-08)
## 
## Iterations: 138
## 
## Tuning parameters:
##       log10(lam)  alpha
## [1,]      -1.488      1
## 
## Log-likelihood: -119.58797
## 
## Omega:
##          [,1]     [,2]     [,3]     [,4]     [,5]
## [1,]  1.80700 -1.02227 -0.12600  0.00000  0.00000
## [2,] -1.02227  2.48516 -1.22929 -0.16928 -0.19100
## [3,] -0.12600 -1.22929  2.66227 -0.92954  0.00000
## [4,]  0.00000 -0.16928 -0.92954  2.57136 -1.13333
## [5,]  0.00000 -0.19100  0.00000 -1.13333  1.95680</code></pre>
<p><br></p>
<p><strong>LASSO:</strong></p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># lasso penalty (default tolerance)</span>
<span class="kw"><a href="../reference/ADMMsigma.html">ADMMsigma</a></span>(X, <span class="dt">alpha =</span> <span class="dv">1</span>)</code></pre></div>
<pre><code>## 
## Call: ADMMsigma(X = X, alpha = 1)
## 
## Iterations: 58
## 
## Tuning parameters:
##       log10(lam)  alpha
## [1,]      -1.488      1
## 
## Log-likelihood: -119.58827
## 
## Omega:
##          [,1]     [,2]     [,3]     [,4]     [,5]
## [1,]  1.80643 -1.02087 -0.12712  0.00000  0.00000
## [2,] -1.02087  2.48166 -1.22585 -0.17014 -0.19098
## [3,] -0.12712 -1.22585  2.65821 -0.92813  0.00000
## [4,]  0.00000 -0.17014 -0.92813  2.57036 -1.13293
## [5,]  0.00000 -0.19098  0.00000 -1.13293  1.95640</code></pre>
<p><br></p>
<p><strong>ELASTIC-NET:</strong></p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># elastic-net penalty (alpha = 0.5)</span>
<span class="kw"><a href="../reference/ADMMsigma.html">ADMMsigma</a></span>(X, <span class="dt">alpha =</span> <span class="fl">0.5</span>)</code></pre></div>
<pre><code>## 
## Call: ADMMsigma(X = X, alpha = 0.5)
## 
## Iterations: 53
## 
## Tuning parameters:
##       log10(lam)  alpha
## [1,]      -1.488    0.5
## 
## Log-likelihood: -115.58376
## 
## Omega:
##          [,1]     [,2]     [,3]     [,4]     [,5]
## [1,]  1.81542 -0.98934 -0.18509  0.00000  0.00000
## [2,] -0.98934  2.44846 -1.16278 -0.20981 -0.20880
## [3,] -0.18509 -1.16278  2.62731 -0.90777  0.00000
## [4,]  0.00000 -0.20981 -0.90777  2.56993 -1.11260
## [5,]  0.00000 -0.20880  0.00000 -1.11260  1.95245</code></pre>
<p><br></p>
<p><strong>RIDGE:</strong></p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ridge penalty</span>
<span class="kw"><a href="../reference/ADMMsigma.html">ADMMsigma</a></span>(X, <span class="dt">alpha =</span> <span class="dv">0</span>)</code></pre></div>
<pre><code>## 
## Call: ADMMsigma(X = X, alpha = 0)
## 
## Iterations: 52
## 
## Tuning parameters:
##       log10(lam)  alpha
## [1,]      -1.488      0
## 
## Log-likelihood: -111.3675
## 
## Omega:
##          [,1]     [,2]     [,3]     [,4]     [,5]
## [1,]  1.82759 -0.97994 -0.25647  0.05735  0.00174
## [2,] -0.97994  2.44836 -1.10788 -0.27060 -0.22860
## [3,] -0.25647 -1.10788  2.62286 -0.90212 -0.00038
## [4,]  0.05735 -0.27060 -0.90212  2.57285 -1.09492
## [5,]  0.00174 -0.22860 -0.00038 -1.09492  1.95196</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ridge penalty no ADMM</span>
<span class="kw"><a href="../reference/RIDGEsigma.html">RIDGEsigma</a></span>(X, <span class="dt">lam =</span> <span class="dv">10</span><span class="op">^</span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">8</span>, <span class="dv">8</span>, <span class="fl">0.01</span>))</code></pre></div>
<pre><code>## 
## Call: RIDGEsigma(X = X, lam = 10^seq(-8, 8, 0.01))
## 
## Tuning parameter:
##       log10(lam)    lam
## [1,]       -2.07  0.009
## 
## Log-likelihood: -119.52862
## 
## Omega:
##          [,1]     [,2]     [,3]     [,4]     [,5]
## [1,]  1.83290 -1.03852 -0.20259  0.07235 -0.00053
## [2,] -1.03852  2.51828 -1.19089 -0.20169 -0.25426
## [3,] -0.20259 -1.19089  2.66369 -0.98993  0.09296
## [4,]  0.07235 -0.20169 -0.98993  2.62894 -1.18140
## [5,] -0.00053 -0.25426  0.09296 -1.18140  1.97550</code></pre>
<p><br></p>
<p>This package also has the capability to provide heat maps for the cross validation errors. The more bright (white) areas of the heat map pertain to more optimal tuning parameters.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># produce CV heat map for ADMMsigma</span>
ADMM =<span class="st"> </span><span class="kw"><a href="../reference/ADMMsigma.html">ADMMsigma</a></span>(X, <span class="dt">tol.abs =</span> <span class="fl">1e-08</span>, <span class="dt">tol.rel =</span> <span class="fl">1e-08</span>)
ADMM <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">plot</span>(<span class="dt">type =</span> <span class="st">"heatmap"</span>)</code></pre></div>
<p><img src="ADMMsigma_files/figure-html/unnamed-chunk-9-1.png" width="700"><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># produce line graph for CV errors for</span>
<span class="co"># ADMMsigma</span>
ADMM <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">plot</span>(<span class="dt">type =</span> <span class="st">"line"</span>)</code></pre></div>
<p><img src="ADMMsigma_files/figure-html/unnamed-chunk-10-1.png" width="700"><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># produce CV heat map for RIDGEsigma</span>
RIDGE =<span class="st"> </span><span class="kw"><a href="../reference/RIDGEsigma.html">RIDGEsigma</a></span>(X, <span class="dt">lam =</span> <span class="dv">10</span><span class="op">^</span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">8</span>, <span class="dv">8</span>, 
    <span class="fl">0.01</span>))
RIDGE <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">plot</span>(<span class="dt">type =</span> <span class="st">"heatmap"</span>)</code></pre></div>
<p><img src="ADMMsigma_files/figure-html/unnamed-chunk-11-1.png" width="700"></p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># produce line graph for CV errors for</span>
<span class="co"># RIDGEsigma</span>
RIDGE <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">plot</span>(<span class="dt">type =</span> <span class="st">"line"</span>)</code></pre></div>
<p><img src="ADMMsigma_files/figure-html/unnamed-chunk-12-1.png" width="700"><br><br></p>
</div>
<div id="simulations" class="section level2">
<h2 class="hasAnchor">
<a href="#simulations" class="anchor"></a>Simulations</h2>
<p>In the simulations below we generated data from a number of oracle precision matrices with various structures. For each data-generating procedure, the <code><a href="../reference/ADMMsigma.html">ADMMsigma()</a></code> function was run using 5-fold cross validation. After 20 replications, the cross validation errors were totalled and the optimal tuning parameters were selected (results in the top figure). These results are compared with the Kullback Leibler (KL) losses between the estimates and the oracle precision matrix (bottom figure). We can see below that our cross validation procedure choosing tuning parameters close to the optimal parameters.</p>
<p><br></p>
<div id="compound-symmetric-p-100-n-50" class="section level3">
<h3 class="hasAnchor">
<a href="#compound-symmetric-p-100-n-50" class="anchor"></a>Compound Symmetric: P = 100, N = 50</h3>
<div class="figure">
<img src="../../../../../../Grad%20School/Research/Rothman/Software/ADMMsigma/vignettes/images/compound_N50_P100.png">
</div>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># oracle precision matrix</span>
Omega =<span class="st"> </span><span class="kw">matrix</span>(<span class="fl">0.9</span>, <span class="dt">ncol =</span> <span class="dv">100</span>, <span class="dt">nrow =</span> <span class="dv">100</span>)
<span class="kw">diag</span>(<span class="dt">Omega =</span> <span class="dv">1</span>)

<span class="co"># generate covariance matrix</span>
S =<span class="st"> </span><span class="kw">qr.solve</span>(Omega)

<span class="co"># generate data</span>
Z =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">100</span> <span class="op">*</span><span class="st"> </span><span class="dv">50</span>), <span class="dt">nrow =</span> <span class="dv">50</span>, <span class="dt">ncol =</span> <span class="dv">100</span>)
out =<span class="st"> </span><span class="kw">eigen</span>(S, <span class="dt">symmetric =</span> <span class="ot">TRUE</span>)
S.sqrt =<span class="st"> </span>out<span class="op">$</span>vectors <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(out<span class="op">$</span>values<span class="op">^</span><span class="fl">0.5</span>) <span class="op">%*%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">t</span>(out<span class="op">$</span>vectors)
X =<span class="st"> </span>Z <span class="op">%*%</span><span class="st"> </span>S.sqrt</code></pre></div>
<p><br></p>
</div>
<div id="compound-symmetric-p-10-n-1000" class="section level3">
<h3 class="hasAnchor">
<a href="#compound-symmetric-p-10-n-1000" class="anchor"></a>Compound Symmetric: P = 10, N = 1000</h3>
<div class="figure">
<img src="../../../../../../Grad%20School/Research/Rothman/Software/ADMMsigma/vignettes/images/compound_N1000_P10.png">
</div>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># oracle precision matrix</span>
Omega =<span class="st"> </span><span class="kw">matrix</span>(<span class="fl">0.9</span>, <span class="dt">ncol =</span> <span class="dv">10</span>, <span class="dt">nrow =</span> <span class="dv">10</span>)
<span class="kw">diag</span>(<span class="dt">Omega =</span> <span class="dv">1</span>)

<span class="co"># generate covariance matrix</span>
S =<span class="st"> </span><span class="kw">qr.solve</span>(Omega)

<span class="co"># generate data</span>
Z =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">10</span> <span class="op">*</span><span class="st"> </span><span class="dv">1000</span>), <span class="dt">nrow =</span> <span class="dv">1000</span>, 
    <span class="dt">ncol =</span> <span class="dv">10</span>)
out =<span class="st"> </span><span class="kw">eigen</span>(S, <span class="dt">symmetric =</span> <span class="ot">TRUE</span>)
S.sqrt =<span class="st"> </span>out<span class="op">$</span>vectors <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(out<span class="op">$</span>values<span class="op">^</span><span class="fl">0.5</span>) <span class="op">%*%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">t</span>(out<span class="op">$</span>vectors)
X =<span class="st"> </span>Z <span class="op">%*%</span><span class="st"> </span>S.sqrt</code></pre></div>
<p><br></p>
</div>
<div id="dense-p-100-n-50" class="section level3">
<h3 class="hasAnchor">
<a href="#dense-p-100-n-50" class="anchor"></a>Dense: P = 100, N = 50</h3>
<div class="figure">
<img src="../../../../../../Grad%20School/Research/Rothman/Software/ADMMsigma/vignettes/images/repsKLdenseQR_N50_P100.png">
</div>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate eigen values</span>
eigen =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1000</span>, <span class="dv">5</span>, <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">100</span> <span class="op">-</span><span class="st"> </span><span class="dv">5</span>)))

<span class="co"># randomly generate orthogonal basis (via QR)</span>
Q =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">100</span><span class="op">*</span><span class="dv">100</span>), <span class="dt">nrow =</span> <span class="dv">100</span>, <span class="dt">ncol =</span> <span class="dv">100</span>) <span class="op">%&gt;%</span><span class="st"> </span>qr <span class="op">%&gt;%</span><span class="st"> </span>qr.Q

<span class="co"># generate covariance matrix</span>
S =<span class="st"> </span>Q <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(eigen) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Q)

<span class="co"># generate data</span>
Z =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">100</span><span class="op">*</span><span class="dv">50</span>), <span class="dt">nrow =</span> <span class="dv">50</span>, <span class="dt">ncol =</span> <span class="dv">100</span>)
out =<span class="st"> </span><span class="kw">eigen</span>(S, <span class="dt">symmetric =</span> <span class="ot">TRUE</span>)
S.sqrt =<span class="st"> </span>out<span class="op">$</span>vectors <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(out<span class="op">$</span>values<span class="op">^</span><span class="fl">0.5</span>) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(out<span class="op">$</span>vectors)
X =<span class="st"> </span>Z <span class="op">%*%</span><span class="st"> </span>S.sqrt</code></pre></div>
<p><br></p>
</div>
<div id="dense-p-10-n-50" class="section level3">
<h3 class="hasAnchor">
<a href="#dense-p-10-n-50" class="anchor"></a>Dense: P = 10, N = 50</h3>
<div class="figure">
<img src="../../../../../../Grad%20School/Research/Rothman/Software/ADMMsigma/vignettes/images/repsKLdense_N50_P10.png">
</div>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate eigen values</span>
eigen =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1000</span>, <span class="dv">5</span>, <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">10</span> <span class="op">-</span><span class="st"> </span><span class="dv">5</span>)))

<span class="co"># randomly generate orthogonal basis (via QR)</span>
Q =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">10</span><span class="op">*</span><span class="dv">10</span>), <span class="dt">nrow =</span> <span class="dv">10</span>, <span class="dt">ncol =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span>qr <span class="op">%&gt;%</span><span class="st"> </span>qr.Q

<span class="co"># generate covariance matrix</span>
S =<span class="st"> </span>Q <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(eigen) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Q)

<span class="co"># generate data</span>
Z =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">10</span><span class="op">*</span><span class="dv">50</span>), <span class="dt">nrow =</span> <span class="dv">50</span>, <span class="dt">ncol =</span> <span class="dv">10</span>)
out =<span class="st"> </span><span class="kw">eigen</span>(S, <span class="dt">symmetric =</span> <span class="ot">TRUE</span>)
S.sqrt =<span class="st"> </span>out<span class="op">$</span>vectors <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(out<span class="op">$</span>values<span class="op">^</span><span class="fl">0.5</span>) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(out<span class="op">$</span>vectors)
X =<span class="st"> </span>Z <span class="op">%*%</span><span class="st"> </span>S.sqrt</code></pre></div>
<p><br></p>
</div>
<div id="tridiagonal-p-100-n-50" class="section level3">
<h3 class="hasAnchor">
<a href="#tridiagonal-p-100-n-50" class="anchor"></a>Tridiagonal: P = 100, N = 50</h3>
<div class="figure">
<img src="../../../../../../Grad%20School/Research/Rothman/Software/ADMMsigma/vignettes/images/repsKLtridiag_N50_P100.png">
</div>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate covariance matrix</span>
<span class="co"># (can confirm inverse is tri-diagonal)</span>
S =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> <span class="dv">100</span>, <span class="dt">ncol =</span> <span class="dv">100</span>)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>){
  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>){
    S[i, j] =<span class="st"> </span><span class="fl">0.7</span><span class="op">^</span><span class="kw">abs</span>(i <span class="op">-</span><span class="st"> </span>j)
  }
}

<span class="co"># generate data</span>
Z =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">10</span><span class="op">*</span><span class="dv">50</span>), <span class="dt">nrow =</span> <span class="dv">50</span>, <span class="dt">ncol =</span> <span class="dv">10</span>)
out =<span class="st"> </span><span class="kw">eigen</span>(S, <span class="dt">symmetric =</span> <span class="ot">TRUE</span>)
S.sqrt =<span class="st"> </span>out<span class="op">$</span>vectors <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(out<span class="op">$</span>values<span class="op">^</span><span class="fl">0.5</span>) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(out<span class="op">$</span>vectors)
X =<span class="st"> </span>Z <span class="op">%*%</span><span class="st"> </span>S.sqrt</code></pre></div>
<p><br><br></p>
</div>
</div>
<div id="benchmark" class="section level2">
<h2 class="hasAnchor">
<a href="#benchmark" class="anchor"></a>Benchmark</h2>
<p>Below we benchmark the various functions contained in <code>ADMMsigma</code>. We can see that <code>ADMMsigma</code> (at the default tolerance) offers comparable computation time to the popular <code>glasso</code> R package.</p>
<div id="computer-specs" class="section level3">
<h3 class="hasAnchor">
<a href="#computer-specs" class="anchor"></a>Computer Specs:</h3>
<ul>
<li>MacBook Pro (Late 2016)</li>
<li>Processor: 2.9 GHz Intel Core i5</li>
<li>Memory: 8GB 2133 MHz</li>
<li>Graphics: Intel Iris Graphics 550</li>
</ul>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate data from tri-diagonal</span>
<span class="co"># (sparse) matrix compute covariance</span>
<span class="co"># matrix (can confirm inverse is</span>
<span class="co"># tri-diagonal)</span>
S =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> <span class="dv">100</span>, <span class="dt">ncol =</span> <span class="dv">100</span>)

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {
    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {
        S[i, j] =<span class="st"> </span><span class="fl">0.7</span><span class="op">^</span>(<span class="kw">abs</span>(i <span class="op">-</span><span class="st"> </span>j))
    }
}

<span class="co"># generate 1000 x 100 matrix with rows</span>
<span class="co"># drawn from iid N_p(0, S)</span>
Z =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">1000</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>), <span class="dt">nrow =</span> <span class="dv">1000</span>, 
    <span class="dt">ncol =</span> <span class="dv">100</span>)
out =<span class="st"> </span><span class="kw">eigen</span>(S, <span class="dt">symmetric =</span> <span class="ot">TRUE</span>)
S.sqrt =<span class="st"> </span>out<span class="op">$</span>vectors <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(out<span class="op">$</span>values<span class="op">^</span><span class="fl">0.5</span>) <span class="op">%*%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">t</span>(out<span class="op">$</span>vectors)
X =<span class="st"> </span>Z <span class="op">%*%</span><span class="st"> </span>S.sqrt


<span class="co"># glasso (for comparison)</span>
<span class="kw">microbenchmark</span>(<span class="kw">glasso</span>(<span class="dt">s =</span> S, <span class="dt">rho =</span> <span class="fl">0.1</span>))</code></pre></div>
<pre><code>## Unit: milliseconds
##                      expr      min       lq     mean   median       uq
##  glasso(s = S, rho = 0.1) 49.46797 50.74574 56.76098 54.42407 59.07784
##       max neval
##  110.3757   100</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># benchmark ADMMsigma - default tolerance</span>
<span class="kw">microbenchmark</span>(<span class="kw"><a href="../reference/ADMMsigma.html">ADMMsigma</a></span>(<span class="dt">S =</span> S, <span class="dt">lam =</span> <span class="fl">0.1</span>, 
    <span class="dt">alpha =</span> <span class="dv">1</span>, <span class="dt">tol.abs =</span> <span class="fl">1e-04</span>, <span class="dt">tol.rel =</span> <span class="fl">1e-04</span>, 
    <span class="dt">trace =</span> <span class="st">"none"</span>))</code></pre></div>
<pre><code>## Unit: milliseconds
##                                                                                           expr
##  ADMMsigma(S = S, lam = 0.1, alpha = 1, tol.abs = 1e-04, tol.rel = 1e-04,      trace = "none")
##      min       lq     mean   median       uq      max neval
##  77.4879 82.67174 87.78936 83.93841 90.41592 137.8903   100</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># benchmark ADMMsigma - tolerance 1e-8</span>
<span class="kw">microbenchmark</span>(<span class="kw"><a href="../reference/ADMMsigma.html">ADMMsigma</a></span>(<span class="dt">S =</span> S, <span class="dt">lam =</span> <span class="fl">0.1</span>, 
    <span class="dt">alpha =</span> <span class="dv">1</span>, <span class="dt">tol.abs =</span> <span class="fl">1e-08</span>, <span class="dt">tol.rel =</span> <span class="fl">1e-08</span>, 
    <span class="dt">trace =</span> <span class="st">"none"</span>))</code></pre></div>
<pre><code>## Unit: milliseconds
##                                                                                           expr
##  ADMMsigma(S = S, lam = 0.1, alpha = 1, tol.abs = 1e-08, tol.rel = 1e-08,      trace = "none")
##       min     lq     mean   median       uq      max neval
##  259.0316 262.03 267.4623 264.1617 269.9908 294.6981   100</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># benchmark ADMMsigma CV - default</span>
<span class="co"># parameter grid</span>
<span class="kw">microbenchmark</span>(<span class="kw"><a href="../reference/ADMMsigma.html">ADMMsigma</a></span>(X, <span class="dt">trace =</span> <span class="st">"none"</span>), 
    <span class="dt">times =</span> <span class="dv">5</span>)</code></pre></div>
<pre><code>## Unit: seconds
##                          expr      min       lq     mean   median       uq
##  ADMMsigma(X, trace = "none") 8.126711 8.132864 8.176253 8.151159 8.234739
##      max neval
##  8.23579     5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># benchmark ADMMsigma parallel CV</span>
<span class="kw">microbenchmark</span>(<span class="kw"><a href="../reference/ADMMsigma.html">ADMMsigma</a></span>(X, <span class="dt">cores =</span> <span class="dv">3</span>, <span class="dt">trace =</span> <span class="st">"none"</span>), 
    <span class="dt">times =</span> <span class="dv">5</span>)</code></pre></div>
<pre><code>## Unit: seconds
##                                     expr     min       lq     mean
##  ADMMsigma(X, cores = 3, trace = "none") 5.66771 5.748002 5.973448
##    median       uq      max neval
##  5.989402 6.159302 6.302825     5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># benchmark ADMMsigma CV - likelihood</span>
<span class="co"># convergence criteria</span>
<span class="kw">microbenchmark</span>(<span class="kw"><a href="../reference/ADMMsigma.html">ADMMsigma</a></span>(X, <span class="dt">crit =</span> <span class="st">"loglik"</span>, 
    <span class="dt">trace =</span> <span class="st">"none"</span>), <span class="dt">times =</span> <span class="dv">5</span>)</code></pre></div>
<pre><code>## Unit: seconds
##                                           expr      min       lq     mean
##  ADMMsigma(X, crit = "loglik", trace = "none") 7.020905 7.088597 7.087975
##    median       uq      max neval
##  7.089398 7.101995 7.138978     5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># benchmark RIDGEsigma CV</span>
<span class="kw">microbenchmark</span>(<span class="kw"><a href="../reference/RIDGEsigma.html">RIDGEsigma</a></span>(X, <span class="dt">lam =</span> <span class="dv">10</span><span class="op">^</span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">8</span>, 
    <span class="dv">8</span>, <span class="fl">0.01</span>), <span class="dt">trace =</span> <span class="st">"none"</span>), <span class="dt">times =</span> <span class="dv">5</span>)</code></pre></div>
<pre><code>## Unit: seconds
##                                                      expr      min
##  RIDGEsigma(X, lam = 10^seq(-8, 8, 0.01), trace = "none") 12.03564
##        lq     mean   median       uq      max neval
##  12.08566 12.18355 12.23003 12.25194 12.31448     5</code></pre>
<p><br></p>
<!-- \begin{thebibliography}{9} -->
<!-- \bibitem{1} -->
<!--   Boyd, Stephen, et al. "Distributed optimization and statistical learning via the alternating direction method of multipliers." Foundations and Trends® in Machine Learning 3.1 (2011): 1-122. -->
<!-- \bibitem{2} -->
<!--   Polson, Nicholas G., James G. Scott, and Brandon T. Willard. "Proximal algorithms in statistics and machine learning." Statistical Science 30.4 (2015): 559-581. -->
<!-- \bibitem{3} -->
<!--   Marjanovic, Goran, and Victor Solo. "On $ l_q $ optimization and matrix completion." IEEE Transactions on signal processing 60.11 (2012): 5714-5724. -->
<!-- \bibitem{4} -->
<!--   Zou, Hui, and Trevor Hastie. "Regularization and variable selection via the elastic net." Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67.2 (2005): 301-320. -->
<!-- \end{thebibliography} -->
</div>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-boyd2011distributed">
<p>Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, and others. 2011. “Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.” <em>Foundations and Trends in Machine Learning</em> 3 (1). Now Publishers, Inc.: 1–122.</p>
</div>
<div id="ref-zou2005regularization">
<p>Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (2). Wiley Online Library: 301–20.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#introduction">Introduction</a></li>
      <li>
<a href="#regularized-precision-matrix-estimation">Regularized Precision Matrix Estimation</a><ul class="nav nav-pills nav-stacked">
<li><a href="#scaled-form-admm">Scaled-Form ADMM</a></li>
      <li><a href="#algorithm">Algorithm</a></li>
      </ul>
</li>
      <li>
<a href="#admmsigma-r-package"><code>ADMMsigma</code> R Package</a><ul class="nav nav-pills nav-stacked">
<li><a href="#installation">Installation</a></li>
      <li><a href="#usage">Usage</a></li>
      <li><a href="#simulations">Simulations</a></li>
      <li><a href="#benchmark">Benchmark</a></li>
      </ul>
</li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Matt Galloway.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
